<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Hwan Heo's personal portfolio website</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Custom Google font-->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@100;200;300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet" />
        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../css/styles.css" rel="stylesheet" />
        <script src="../../js/mathjax-config.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({            
                tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}            
            });
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7ETSKPK9"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-RF7ETSKPK9');
        </script>
    </head>
    <body class="d-flex flex-column h-100 bg-light">
        <main class="flex-shrink-0">
            <!-- Navigation-->
            <nav class="navbar navbar-expand-lg navbar-light bg-white py-3">
                <div class="container px-5">
                    <a class="navbar-brand" href="../../"><span class="fw-bolder text-primary">HwanHeo's log</span></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ms-auto mb-2 mb-lg-0 small fw-bolder">
                            <li class="nav-item"><a class="nav-link" href="../../">Home</a></li>
                            <!--li class="nav-item"><a class="nav-link" href="../../resume.html">Resume</a></li-->
                            <li class="nav-item"><a class="nav-link" href="../">Projects</a></li>
                            <li class="nav-item"><a class="nav-link" href="../../blogs/"> | &nbsp; &nbsp; Blog</a></li>
                            <!--li class="nav-item"><a class="nav-link" href="../../contact.html">Contact</a></li-->
                        </ul>
                    </div>
                </div>
            </nav>
            <!-- Projects Section-->
            <section class="py-5">
                <div class="container px-5 mb-5">
                    <div class="text-center mb-5">
                        <h1 class="display-5 fw-bolder mb-0"><span class="text-gradient d-inline">Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild</span></h1>
                        <div class="fs-3 fw-light text-muted"> preprint </div>
                        <div class="fs-3 fw-light text-muted"> Korea University </div>
                    </div>
                    <div class="row gx-5 justify-content-center">
                        <div class="col-lg-11 col-xl-9 col-xxl-8">
                            <!--img class="img-fluid" src="./assets/poster.png" /-->
                            <h2> Overview </h2>
                            <hr/>
                            <p>
                                This research introduces <strong>SF-NeRF</strong>, an advanced framework designed to reconstruct neural scene representations 
                                from a limited number of unconstrained tourist photos. 
                            </p>
                            <p> SF-NeRF focus on decomposing the transient and static phenomena, 
                                predicting the transient components of each image with an additional MLP module dubbed FilterNet. 
                                FilterNet exploits semantic information given by an image encoder pretrained in an unsupervised manner, 
                                which is the key to achieve few-shot learning. </p>
                            <ul>
                                <li> present a learning framework for reconstructing neural scene representations from a small number of unconstrained (different illuminations, occlusions, etc) tourist photos </li>
                                <li> Using DINO (semantic segmentation) features, we make occlusion filtering module that predicts the transient color and its opacity for each pixel </li>
                            </ul>
                            <br/>
                            <h2> Methodology </h2>
                            <hr/>
                            <h3>Neural Radiance Fields</h3>
                            <p>
                                The core NeRF model represents a scene as a continuous volumetric radiance field $F_\theta$, 
                                which maps a 3D position $ \mathbf{x} $ and viewing direction $ \mathbf{d} $ to a color $ \mathbf{c} $ and density $ \sigma $:
                            </p>
                            <div class="math-container">
                                $$
                                [\sigma, \mathbf{z}] = \text{MLP}_{\theta_1}(\gamma_{\mathbf{x}}(\mathbf{x})), \quad \mathbf{c} = \text{MLP}_{\theta_2}(\gamma_{\mathbf{d}}(\mathbf{d}), \mathbf{z})
                                $$
                            </div>
                            <p>
                                where $\gamma_{\mathbf{x}} $ and $ \gamma_{\mathbf{d}} $ are positional encoding functions.
                            </p>
  
                            <p>
                                The expected color $C(\mathbf{r})$ along a camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ is computed using volume rendering:
                            </p>
                            <div class="math-container">
                                $$ C(\mathbf{r}) = \sum_{k=1}^{K} T(t_k) \alpha(t_k) \mathbf{c}(t_k)
                                $$
                            </div>
                            <p>
                                where $\alpha(t_k) = 1 - \exp(-\sigma(t_k) \delta_k) $ and $T(t_k) = \exp\left(-\sum_{k'=1}^{k-1} \sigma(t_{k'}) \delta_{k'}\right)$.
                            </p><br/>
                            <h3> NeRF w/ Latent Embeding</h3>
                            <p>
                                To make more robust representation for the wild images, the radiance and density can be conditioned on an appearance embedding $ \mathbf{l}_i $ for each image $I_i$:
                            </p>
                            <div class="math-container">
                                $$
                                \mathbf{c}_i = \text{MLP}_{\theta_2}(\gamma_{\mathbf{d}}(\mathbf{d}), \mathbf{z}, \mathbf{l}_i), \quad C_i(\mathbf{r}) = \sum_{k=1}^{K} T(t_k) \alpha(t_k) \mathbf{c}_i(t_k)
                                $$
                            </div>
                            <p>
                                The purpose of the embedding is to capture and represent the variations in appearance and lighting conditions across different images of the same scene. 
                                These variations can include changes in illumination, color tone, and other environmental factors that differ from image to image.
                            </p><br/>
                            <h3> Dientangling Transients and Statics Using FilterNet</h3>
                            <img class="img-thumbnail" src="assets/main.png" width="100%" />
                            <p>
                                To achieve consistent scene decomposition, we use an additional MLP module, dubbed FilterNet.
                                FilterNet receives feature maps $f_i(\mathbf{p})$ extracted from the input image by a pretrained <strong>DINO</strong> image encoder $E_\phi$. 
                                These features help FilterNet predict transient objects from images.
                            </p>
                            <p>
                                Using Filternet, we model transient phenomena by learning image-dependent 2D maps: transient RGBA and uncertainty maps.
                                The final predicted pixel color $\hat{C}_i(\mathbf{r})$ is obtained by blending the static color $C_i(\mathbf{r})$ and transient color:
                            </p>
                            <div class="math-container">
                                $$ \hat{C}_i(\mathbf{r}) = \alpha_i^{(\tau)}(\mathbf{p}_r) C_i^{(\tau)}(\mathbf{p}_r) + \left(1 - \alpha_i^{(\tau)}(\mathbf{p}_r)\right) C_i(\mathbf{r})
                                $$
                            </div>
                            <p>
                                where $ \alpha_i^{(\tau)}(\mathbf{p}_r)$ is the transient opacity, $C_i^{(\tau)}(\mathbf{p}_r)$ is the transient color, and $\mathbf{p}_r$ is the pixel corresponding to ray $\mathbf{r} $.
                            </p>

                            <p>
                                For each ray $\mathbf{r}$ in image $\mathcal{I}_i$, we train FilterNet to disentangle transient components from the scene in an unsupervised manner with loss $\mathcal{L}^{(i)}_\text{t}$:
                            </p>
                            <div class="math-container">
                                $$ 
                                    \mathcal{L}^{(i)}_\text{t}(\mathbf{r}) = \frac{\big\lVert \hat{\mathbf{C}}_i(\mathbf{r})- \bar{\mathbf{C}}_i(\mathbf{r})\big\rVert^2_2}{2\beta_i(\mathbf{r})^2} + \frac{\log\beta_i(\mathbf{r})^2}{2} + \lambda_\alpha \alpha^{(\tau)}_i\hspace{-0.05cm}(\mathbf{r}),
                                $$
                            </div>
                            <p>
                                where $\bar{\mathbf{C}}$ is the ground-truth color. 
                                
                                The first and second terms can be viewed as the negative log likelihood of $\bar{\mathbf{C}}_i(\mathbf{r})$ 
                                which is assumed to follow an isotropic normal distribution with mean $\hat{\mathbf{C}}_i(\mathbf{r})$ 
                                and variance $\beta_i(\mathbf{r})^2$.
                                The third term discourages \jaewon{FilterNet} from describing static phenomena.
                            </p><br/>

                            <h4 id="comparison-to-latent-nerf-and-nerf-w">Remark. Comparison to Latent NeRF and NeRF-W</h4>
                            <ul>
                            <li><strong>Latent NeRF</strong><br/> primarily uses appearance embeddings to model varying lighting and view-dependent effects. However, it does not specifically address the challenge of transient occlusions or moving objects, making it less effective in dynamic real-world scenes.</li>
                            <br/>
                            <li><strong>NeRF-W</strong><br/> improves on Latent NeRF by introducing a transient head to handle scene variations. However, it requires a significant amount of training data to separate transient and static components effectively, limiting its applicability in few-shot scenarios.</li>
                            </ul>
                            <p><strong>SF-NeRF</strong> overcomes these limitations by integrating semantic guidance through FilterNet, which enables an effective decomposition of static and transient components even with minimal training images. The reparameterization and smoothness techniques further enhance its ability to produce clear and accurate scene reconstructions without the artifacts that commonly affect Latent NeRF and NeRF-W in challenging conditions.</p>


                            <p> <br/> </p>
                            <h2>Experiments</h2>
                            <hr/>
                            <p> The overall quantitative results and qualitative results demonstrate that the SF-NeRF mostly outperforms the baselines in the few-shot setting in the wild images. </p>
                            <p>
                                SF-NeRF significantly outperforms NeRF-W and Latent NeRF in novel view synthesis tasks, particularly in few-shot settings. Evaluations on the Phototourism dataset—featuring diverse and unconstrained photographs—demonstrate SF-NeRF's superior ability to handle transient occluders and achieve high-quality scene reconstructions with just 30 images per landmark.
                            </p>
                            <div style="text-align : center;">
                                <img class="img-thumbnail" src="assets/qual.png" width="100%">
                            </div>
                            <div style="text-align : center;">
                                <img class="img-thumbnail" src="assets/qual2.png" width="100%">
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>
        <!-- Footer-->
        <footer class="bg-white py-4 mt-auto">
            <div class="container px-5">
                <div class="row align-items-center justify-content-between flex-column flex-sm-row">
                    <div class="col-auto"><div class="small m-0">Copyright &copy; hwanheo's log</div></div>
                    <div class="col-auto">
                        <a class="small" href="gjghks950@naver.com">Contact</a>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>