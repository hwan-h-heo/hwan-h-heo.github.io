<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="../../assets/favicon.ico" rel="icon">
  <link href="../../assets/favicon.ico" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect">
  <link rel="preconnect" href="<https://fonts.googleapis.com>">
  <link rel="preconnect" href="<https://fonts.gstatic.com>">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="../../assets/css/used.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
  
  <script src="../../js/mathjax-config.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({            
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}            
        });
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
        }
        th, td {
            width: 50%;
            border: none;
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2; /* 옅은 회색 */
        }
        .progress-bar {
          height: 0.4rem;
          background: #6EA8FE;
          width: 0%;
          z-index: 9999;
          position: fixed;
        }
    </style>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7ETSKPK9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-RF7ETSKPK9');
    </script>
</head>
<div class="progress-container">
  <div class="progress-bar" id="myBar"></div>
</div>
<body class="portfolio-details-page">

  <header id="header" class="header dark-background d-flex flex-column">
    <i class="header-toggle d-xl-none bi bi-list"></i>

    <div class="profile-img">
      <img src="../../assets/icon.webp" alt="" class="img-fluid rounded-circle">
    </div>

    <a href="index.html" class="logo d-flex align-items-center justify-content-center">
      <h1 class="sitename">Hwan Heo</h1>
    </a>

    <div class="social-links text-center">
      <a href="https://github.com/hwanhuh" class="github"><i class="bi bi-github"></i></a>
      <a href="https://www.linkedin.com/in/hwan-heo-0905korea/" class="linkedin"><i class="bi bi-linkedin"></i></a>
      <a href="https://scholar.google.com/citations?user=RulvYTkAAAAJ" class="instagram"><i class="ai ai-google-scholar"></i></i></a>
      <a href="mailto:gjghks950@naver.com" class="google-plus"><i class="bi bi-envelope"></i></a>
    </div>

    <nav id="navmenu" class="navmenu">
      <ul>
        <li><a href="../../#home"><i class="bi bi-house navicon"></i>Home</a></li>
        <li><a href="../../#about"><i class="bi bi-person navicon"></i> About</a></li>
        <li><a href="../../#resume"><i class="bi bi-file-earmark-text navicon"></i> Resume</a></li>
        <li><a href="../../#portfolio" class="active"><i class="bi bi-images navicon"></i> Portfolio</a></li>
        <li><a href="../../#blog"><i class="bi bi-keyboard navicon"></i> Blog </a></li>
      </ul>
    </nav>
  </header>

  <main class="main">

    <!-- Page Title -->
    <div class="page-title dark-background">
      <div class="container d-lg-flex justify-content-between align-items-center">
        <!-- <h1 class="mb-2 mb-lg-0">Title</h1> -->
        <nav class="breadcrumbs">
          <ol>
            <li><a href="../../">Home</a></li>
            <li class="current">Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild</li>
          </ol>
        </nav>
      </div>
    </div><!-- End Page Title -->

    <!-- Portfolio Details Section -->
    <section id="portfolio-details" class="portfolio-details section">
        <div class="row gx-5 justify-content-center text-center mb-5">
          <div class="col-9 col-lg-8 col-xl-7 col-xxl-6">
            <h1 class="display-6 fw-bolder mb-0"><span class="text-gradient d-inline">Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild</span></h1>
            <div class="fs-3 fw-light text-muted"> preprint <a href="https://arxiv.org/abs/2303.03966" class="bi bi-link-45deg"></a></div>
            <div class="fs-3 fw-light text-muted"> Korea University </div>
          </div>
        </div>
        <div class="row gx-5 justify-content-center">
            <div class="col-9 col-lg-8 col-xl-7 col-xxl-6">
                <div class="fs-6"> <strong>TL; DR:</strong> We introduce <strong>SF-NeRF</strong>, an advanced framework for reconstructing neural scene representations from a small number of unconstrained in-the-wild photos. </div>
                <div class="fs-6"> <strong>Keywords:</strong> Neural Rendering, Radiance Fields, NeRF in-the-wild</div>
                <p> <br/> </p>
                <h2> Overview </h2>
                <hr/>
                <p> 
                    SF-NeRF tackles the challenge of decomposing transient and static phenomena in neural scene reconstruction. 
                    At its core is <em>FilterNet</em>, an additional MLP module designed to predict the transient components of each image. 
                    FilterNet leverages semantic information extracted from an image encoder pretrained in an unsupervised manner, which is key to enabling few-shot learning.
                </p>
                <p>
                    Our framework is tailored for scenes captured under unconstrained conditions, such as varying illuminations and occlusions, commonly found in tourist photos. By utilizing DINO-based semantic features, we introduce an occlusion filtering module that predicts the transient color and its opacity for each pixel, enabling robust neural scene reconstruction even with limited data.
                </p>
                <br/>
                <h2> Methodology </h2>
                <hr/>
                <h3>Neural Radiance Fields</h3>
                <p>
                    The core NeRF model represents a scene as a continuous volumetric radiance field $F_\theta$, 
                    which maps a 3D position $ \mathbf{x} $ and viewing direction $ \mathbf{d} $ to a color $ \mathbf{c} $ and density $ \sigma $:
                </p>
                <div class="math-container">
                    $$
                    [\sigma, \mathbf{z}] = \text{MLP}_{\theta_1}(\gamma_{\mathbf{x}}(\mathbf{x})), \quad \mathbf{c} = \text{MLP}_{\theta_2}(\gamma_{\mathbf{d}}(\mathbf{d}), \mathbf{z})
                    $$
                </div>
                <p>
                    where $\gamma_{\mathbf{x}} $ and $ \gamma_{\mathbf{d}} $ are positional encoding functions.
                </p>

                <p>
                    The expected color $C(\mathbf{r})$ along a camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ is computed using volume rendering:
                </p>
                <div class="math-container">
                    $$ C(\mathbf{r}) = \sum_{k=1}^{K} T(t_k) \alpha(t_k) \mathbf{c}(t_k)
                    $$
                </div>
                <p>
                    where $\alpha(t_k) = 1 - \exp(-\sigma(t_k) \delta_k) $ and $T(t_k) = \exp\left(-\sum_{k'=1}^{k-1} \sigma(t_{k'}) \delta_{k'}\right)$.
                </p><br/>
                <h3> NeRF w/ Latent Embeding</h3>
                <p>
                    To make more robust representation for the wild images, the radiance and density can be conditioned on an appearance embedding $ \mathbf{l}_i $ for each image $I_i$:
                </p>
                <div class="math-container">
                    $$
                    \mathbf{c}_i = \text{MLP}_{\theta_2}(\gamma_{\mathbf{d}}(\mathbf{d}), \mathbf{z}, \mathbf{l}_i), \quad C_i(\mathbf{r}) = \sum_{k=1}^{K} T(t_k) \alpha(t_k) \mathbf{c}_i(t_k)
                    $$
                </div>
                <p>
                    The purpose of the embedding is to capture and represent the variations in appearance and lighting conditions across different images of the same scene. 
                    These variations can include changes in illumination, color tone, and other environmental factors that differ from image to image.
                </p><br/>
                <h3> Dientangling Transients and Statics Using FilterNet</h3>
                <img class="img-thumbnail" src="assets/main.png" width="100%" />
                <p>
                    To achieve consistent scene decomposition, we use an additional MLP module, dubbed FilterNet.
                    FilterNet receives feature maps $f_i(\mathbf{p})$ extracted from the input image by a pretrained <strong>DINO</strong> image encoder $E_\phi$. 
                    These features help FilterNet predict transient objects from images.
                </p>
                <p>
                    Using Filternet, we model transient phenomena by learning image-dependent 2D maps: transient RGBA and uncertainty maps.
                    The final predicted pixel color $\hat{C}_i(\mathbf{r})$ is obtained by blending the static color $C_i(\mathbf{r})$ and transient color:
                </p>
                <div class="math-container">
                    $$ \hat{C}_i(\mathbf{r}) = \alpha_i^{(\tau)}(\mathbf{p}_r) C_i^{(\tau)}(\mathbf{p}_r) + \left(1 - \alpha_i^{(\tau)}(\mathbf{p}_r)\right) C_i(\mathbf{r})
                    $$
                </div>
                <p>
                    where $ \alpha_i^{(\tau)}(\mathbf{p}_r)$ is the transient opacity, $C_i^{(\tau)}(\mathbf{p}_r)$ is the transient color, and $\mathbf{p}_r$ is the pixel corresponding to ray $\mathbf{r} $.
                </p>

                <p>
                    For each ray $\mathbf{r}$ in image $\mathcal{I}_i$, we train FilterNet to disentangle transient components from the scene in an unsupervised manner with loss $\mathcal{L}^{(i)}_\text{t}$:
                </p>
                <div class="math-container">
                    $$ 
                        \mathcal{L}^{(i)}_\text{t}(\mathbf{r}) = \frac{\big\lVert \hat{\mathbf{C}}_i(\mathbf{r})- \bar{\mathbf{C}}_i(\mathbf{r})\big\rVert^2_2}{2\beta_i(\mathbf{r})^2} + \frac{\log\beta_i(\mathbf{r})^2}{2} + \lambda_\alpha \alpha^{(\tau)}_i\hspace{-0.05cm}(\mathbf{r}),
                    $$
                </div>
                <p>
                    where $\bar{\mathbf{C}}$ is the ground-truth color. 
                    
                    The first and second terms can be viewed as the negative log likelihood of $\bar{\mathbf{C}}_i(\mathbf{r})$ 
                    which is assumed to follow an isotropic normal distribution with mean $\hat{\mathbf{C}}_i(\mathbf{r})$ 
                    and variance $\beta_i(\mathbf{r})^2$.
                    The third term discourages \jaewon{FilterNet} from describing static phenomena.
                </p><br/>

                <h4 id="comparison-to-latent-nerf-and-nerf-w">Remark. Comparison to Latent NeRF and NeRF-W</h4>
                <ul>
                <li><strong>Latent NeRF</strong><br/> primarily uses appearance embeddings to model varying lighting and view-dependent effects. However, it does not specifically address the challenge of transient occlusions or moving objects, making it less effective in dynamic real-world scenes.</li>
                <br/>
                <li><strong>NeRF-W</strong><br/> improves on Latent NeRF by introducing a transient head to handle scene variations. However, it requires a significant amount of training data to separate transient and static components effectively, limiting its applicability in few-shot scenarios.</li>
                </ul>
                <p><strong>SF-NeRF</strong> overcomes these limitations by integrating semantic guidance through FilterNet, which enables an effective decomposition of static and transient components even with minimal training images. The reparameterization and smoothness techniques further enhance its ability to produce clear and accurate scene reconstructions without the artifacts that commonly affect Latent NeRF and NeRF-W in challenging conditions.</p>


                <p> <br/> </p>
                <h2>Experiments</h2>
                <hr/>
                <p> The overall quantitative results and qualitative results demonstrate that the SF-NeRF mostly outperforms the baselines in the few-shot setting in the wild images. </p>
                <p>
                    SF-NeRF significantly outperforms NeRF-W and Latent NeRF in novel view synthesis tasks, particularly in few-shot settings. Evaluations on the Phototourism dataset—featuring diverse and unconstrained photographs—demonstrate SF-NeRF's superior ability to handle transient occluders and achieve high-quality scene reconstructions with just 30 images per landmark.
                </p>
                <div style="text-align : center;">
                    <img class="img-thumbnail" src="assets/qual.png" width="100%">
                </div>
                <div style="text-align : center;">
                    <img class="img-thumbnail" src="assets/qual2.png" width="100%">
                </div>
            </div>
        </div>
    </section><!-- /Portfolio Details Section -->

  </main>

  <footer id="footer" class="footer position-relative light-background">

    <div class="container">
      <div class="copyright text-center ">
        <p>© <span>Copyright</span> <strong class="px-1 sitename">Hwan Heo</strong> <span>All Rights Reserved</span></p>
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../../assets/vendor/php-email-form/validate.js"></script>
  <script src="../../assets/vendor/aos/aos.js"></script>
  <script src="../../assets/vendor/typed.js/typed.umd.js"></script>
  <script src="../../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../../assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="../../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../../assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="../../assets/js/main.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        const observer = new IntersectionObserver(() => {
            updateScrollIndicator();
        });

        document.querySelectorAll("model-viewer").forEach((viewer) => observer.observe(viewer));

        window.onscroll = updateScrollIndicator;
        window.onresize = updateScrollIndicator;

        function updateScrollIndicator() {
            var winScroll = document.documentElement.scrollTop;
            var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            if (height > 0) {
                var scrolled = (winScroll / height) * 100;
                document.getElementById("myBar").style.width = scrolled + "%";
            }
        }
    });
  </script>

</body>

</html>