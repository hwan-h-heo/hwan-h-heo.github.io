<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Hwan Heo's personal portfolio website</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="../../assets/favicon.ico" />
        <!-- Custom Google font-->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@100;200;300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet" />
        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../css/styles.css" rel="stylesheet" />
        <style>
            table {
                width: 100%;
                border-collapse: collapse;
            }
            th, td {
                width: 50%;
                border: 1px solid #ddd;
                padding: 10px;
                text-align: center;
                vertical-align: middle;
            }
            th {
                width: 15%; /* Reduced width of the header column */
                background-color: #f2f2f2; 
            }
        </style>
        <script src="../../js/mathjax-config.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({            
                tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}            
            });
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7ETSKPK9"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-RF7ETSKPK9');
        </script>
    </head>
    <body class="d-flex flex-column h-100 bg-light">
        <main class="flex-shrink-0">
            <!-- Navigation-->
            <nav class="navbar navbar-expand-lg navbar-light bg-white py-3">
                <div class="container px-5">
                    <a class="navbar-brand" href="../../"><span class="fw-bolder text-primary">HwanHeo's log</span></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ms-auto mb-2 mb-lg-0 small fw-bolder">
                            <li class="nav-item"><a class="nav-link" href="../../">Home</a></li>
                            <!--li class="nav-item"><a class="nav-link" href="../../resume.html">Resume</a></li-->
                            <li class="nav-item"><a class="nav-link" href="../">Projects</a></li>
                            <li class="nav-item"><a class="nav-link" href="../../blogs/"> | &nbsp; &nbsp; Blog</a></li>
                            <!--li class="nav-item"><a class="nav-link" href="../../contact.html">Contact</a></li-->
                        </ul>
                    </div>
                </div>
            </nav>
            <!-- Projects Section-->
            <section class="py-5">
                <div class="container px-5 mb-5">
                    <div class="text-center mb-5">
                        <h1 class="display-5 fw-bolder mb-0"><span class="text-gradient d-inline"> Accelerated Face NeRF <br/> with Multi-Camera Captures </span></h1>
                        <div class="fs-3 fw-light text-muted"> NC Research </div>
                    </div>
                    <div class="row gx-5 justify-content-center">
                        <div class="col-lg-11 col-xl-9 col-xxl-8">
                            <div class="fs-6"> <strong>TL; DR:</strong> This project presents an accelerated facial NeRF pipeline utilizing multi-camera setups to capture multi-view data for novel view synthesis. </div>
                            <div class="fs-6"> <strong>Keywords:</strong> Facial NeRF, Multi-Camera Setup, 3D Morphable Model (3DMM) </div>
                            <p> <br/> </p>
                            <h2> Overview </h2>
                            <hr/>
                            <p>
                                This project introduces an accelerated method for face NeRF using multi-camera captures to gather multi-view data for novel view synthesis (NVS). By utilizing a feed-forward approach to estimate 3D Morphable Model (3DMM) parameters, we significantly reduce preprocessing times compared to traditional methods. We further adapt the reconstructed mesh for efficient ray-casting in a perspective projection, optimizing both speed and accuracy for facial reconstruction tasks.
                            </p>
                            <p> <br/> </p>

                            <h2> Key Structures </h2>
                            <hr/>
                            <h3> Multi-Camera Face Captures </h3>
                            <DIV style ="text-align:center";>
                                <img class="img-fluid" src="assets/aligned_mesh_vis.jpg" width="60%" />
                            </DIV>
                            <p> In traditional neural radiance field (NeRF) setups, monocular portrait synthesis has been a common approach, often limited by the reliance on single-view data, which constrains the performance of novel view synthesis (NVS). Our project addresses this limitation by employing multi-camera captures, enabling us to gather richer, multi-view data essential for improved NVS performance. </p>
                            <DIV style ="text-align:center";>
                                <img class="img-fluid" src="assets/cam_vis_mediapipe_lmks2.jpg" width="100%" />
                            </DIV>
                            <p> <br/> </p>

                            <h3> Fast Feed-Forward Face Blendshape Prediction </h3>
                            <DIV style ="text-align:center";>
                                <img class="img-fluid" src="https://zielon.github.io/assets/img/mica/teaser.jpg" width="100%" />
                            </DIV>
                            <p>
                                To efficiently estimate the 3D Morphable Model (3DMM) parameters, we implemented a feed-forward approach. 
                                While methods like MICA utilize a frame-wise, mesh-based differentiable rendering pipeline for parameter estimation, this technique, though accurate, is computationally expensive, requiring over 15 hours of preprocessing for just 3-5 minutes of video. 
                            </p>
                            <p>
                                Given that extreme precision in 3DMM parameter estimation is not critical for our purposes, we adopted the faster feed-forward method. This allows us to trade off some reconstruction accuracy while dramatically reducing the preprocessing time to just a few minutes for the same input video duration.
                            </p>
                            <p>
                                Below is the comparison between famouse optimization-based 3DMM estimation method: <a href="https://zielon.github.io/mica/">MICA</a>  and feed-forward-based method: <a href="https://emoca.is.tue.mpg.de/">EMOCA</a>. (Data: a famouse president obama)
                            </p>
                            <table>
                                <tr>
                                    <th>MICA</th>
                                    <th>EMOCA</th>
                                    <th>Comparison</th>
                                </tr>
                            </table>
                            <div style="text-align: center;">
                                <video style="width: 100%" muted autoplay playsinline loop>
                                    <source src="assets/obm-ezgif.com-resize-video.mp4" type="video/mp4">
                                </video>
                            </div>
                            <p> <br/> </p>
                            <h3> Mesh Adaptation for Multi-Camera System </h3>
                            <p>
                                Once the 3DMM parameters are estimated, we construct a 3D mesh that serves as a proxy for ray-casting operations. 
                                The challenge here lies in adapting the pre-trained network, which was trained under the assumption of an orthographic camera model. 
                            </p>
                            <p>
                                Since we are working within a perspective projection framework, we reverse-projected the mesh to fit how it would appear under a perspective camera model. 
                                Using this back-projection, we construct a coarse mesh in the world coordinate system, which serves as the basis for building a bounding volume hierarchy (BVH) to facilitate efficient ray-casting during rendering.
                            </p>
                            <p>
                                Below is a visualization of our back-projected canonical mesh. The first column displays the canonical mesh used for 3DMM estimation, while the third through fifth columns show the back-projected canonical mesh from different camera perspectives. 
                            </p>

                            <table>
                                <tr>
                                    <th>Canonical</th>
                                    <th>EMOCA</th>
                                    <th>Cam #1</th>
                                    <th>Cam #2</th>
                                    <th>Cam #3</th>
                                </tr>
                            </table>
                            <div style="text-align: center;">
                                <video style="width: 100%" muted autoplay playsinline loop>
                                    <source src="assets/hwan_vis_mesh_pose_multicam-ezgif.com-crop-video.mp4" type="video/mp4">
                                </video>
                            </div>
                            <p> <br/> </p>
                            
                            <h3> Reconstructed Neural Portrait </h3>
                            <hr/>
                            <p>
                                Below is the reconstructed neural portrait result. Unlike monocular reconstruction algorithms, our method correctly captures torso movement as well. The use of multi-view images obtained from multi-camera setups ensures high visual fidelity in the output. Additionally, since the structure is based on predicting face deformations through 3DMM parameters, the reconstructed neural avatar can be manipulated for further adjustments.
                            </p>
                            <table>
                                <tr>
                                    <th>Reconstructed Neural Portrait</th>
                                    <th>Novel View Synthesis</th>
                                </tr>
                            </table>
                            <div style="text-align: center;">
                                <video style="width: 100%" muted autoplay playsinline loop>
                                    <source src="assets/hwan_nerf.mp4" type="video/mp4">
                                </video>
                            </div>
                            <hr/>
                            <p>
                                This project successfully integrates multi-camera setups with accelerated facial NeRF, showcasing the potential for efficient and high-fidelity facial reconstruction. By leveraging feed-forward methods for 3DMM estimation and adapting ray-casting for perspective projections, we achieve a significant reduction in preprocessing time while maintaining high visual fidelity.
                            </p>
                        </div>
                    </div>
                </div>
            </section>
        </main>
        <!-- Footer-->
        <footer class="bg-white py-4 mt-auto">
            <div class="container px-5">
                <div class="row align-items-center justify-content-between flex-column flex-sm-row">
                    <div class="col-auto"><div class="small m-0">Copyright &copy; Hwan Heo</div></div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
