<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title> Why Positional Encoding Makes NeRF more Powerful </title>
        <link rel="icon" type="image/x-icon" href="../../../assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../../css/blog_style.css" rel="stylesheet" />
        <!-- Prism.js CSS -->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css" rel="stylesheet" />
        <style>
            table {
                width: 100%;
                border-collapse: collapse;
            }
            th, td {
                width: 50%;
                border: 1px solid #ddd;
                padding: 10px;
                text-align: center;
                vertical-align: middle;
            }
            th {
                width: 15%; /* Reduced width of the header column */
                background-color: #f2f2f2; /* 옅은 회색 */
            }
            .img-fluid {
                max-width: 100%;
                height: auto;
                display: block;
                margin: auto;
            }
        </style>
        <!-- MathJax CSS -->
        <script src="../../../js/mathjax-config.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({            
                tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}            
            });
        </script>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

        <script src="../../../js/include.js"></script>

        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7ETSKPK9"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-RF7ETSKPK9');
        </script>
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="../../">HwanHeo's Blog</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="../../">Post</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="../../../">HwanHeo's log</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('../../../assets/blog_bg.jpeg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1> Why Positional Encoding Makes NeRF more Powerful</h1>
                            <br/>
                            <span class="meta">
                                Posted by
                                Hwan Heo
                                on November 28, 2021
                            </span>
                            <div style="text-align: center;">
                                <button type="button" class="btn custom-btn" onclick="setLanguage('eng')" style="font-size: 13px;">eng</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <nav class="toc">
                <ul>
                    <li><a href="#intro"> Introduction </a></li>
                    <li><a href="#sec2"> Background </a></li>
                    <ul>
                        <li><a href="#sec2.1"> Kernel Trick </a></li>
                        <li><a href="#sec2.2"> Neural Tangent Kernel </a></li>
                        <li><a href="#sec2.3"> Spectral Bias of Neural Networks </a></li>
                    </ul>
                    <li>
                        <a href="#sec3"> Fourier Featuring </a>
                    </li>
                    <ul>
                        <li><a href="#sec3.1"> Fourier Feature Mapping </a></li>
                        <li><a href="#sec3.2"> Fourier Featuring w/ NTK </a></li>
                    </ul>
                </ul>
            </nav>
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div data-include-path="../layout/series.html"></div>
                        <br/>
                        <h3 id="tl-dr">TL;DR</h3>
                        <p> In this article, we explore why positional encoding increases NeRF's high-fidelity reconstruction ability via exploring the paper: <em>Fourier Features Let Networks Learn High-Frequency Functions in Low-Dimensional Domains </em> </p>
                        <p> By leveraging a Neural Tangent Kernel (NTK) theory, the authors demonstrate that Fourier features improve the convergence and performance of neural networks on these complex tasks.</p>
                        <h2 id="intro">1. Introduction</h2>
                        <p>Fourier-featuring is a function that embeds a coordinate space point into frequency space.</p>
                        <p>A prominent example in deep learning is <strong>&#39;Positional Encoding&#39;</strong>, which uses sinusoidal functions to embed coordinate space into frequency space, thereby incorporating positional information that Networks cannot capture.</p>
                        <figure class="justify-content-center">
                            <img src="assets/main.png" alt="Gaussian RT" width="100%">
                            <figcaption style="text-align: center; font-size: 15px;"> <strong>Figure 1.</strong> Coordinate-Based MLPs </figcaption>
                        </figure>
                        <p>Building upon <strong>NTK theory</strong>, this article foucuses on the theoretical investigation of how neural networks process coordinate information through Fourier-featuring, 
                            especially for the coordinate-based MLPs, which map dense, continuous low-dimensional input to the high-dimensional output (<em>e.g.,</em> NeRF). 
                        </p>
                        <h3 id="sec2">2. Background </h3><br/>
                        <h3 id="sec2.1">2.1. Kernel Trick</h3>
                        <figure class="justify-content-center">
                            <img src="assets/kernel_trick.png" alt="Gaussian RT" width="80%">
                            <figcaption style="text-align: center; font-size: 15px;"> <strong>Figure 2.</strong> illustration of Kernel-Trick </figcaption>
                        </figure>
                        <p>For a linearly inseparable data point $x$, let $\phi (x)$ be a non-linear mapping function that makes $\phi (x)$ linearly separable.</p>
                        <p>The kernel trick performs kernel regression without explicitly finding the feature map by defining the kernel as follows:</p>
                        <p>$$K(x, \ x&#39;) = \phi(x) ^T \phi(x&#39;)
                        $$</p>
                        <p>This approach is interpreted as using a feature map $\phi$ with desirable properties through the kernel, rather than mapping input $x$ and then taking the inner product.</p>
                        <h3 id="sec2.2">2.2. Neural Tangent Kernel </h3>
                        <p><strong><em>Neural Tangent Kernel (NTK)</em></strong> theory describes the gradient descent-based training of deep neural networks with infinite width through kernel regression, aiming to explain neural networks using the kernel trick.</p>
                        <h3 id="2-2-1-linearization-of-nn-training-kernel-definition"> Linearization of NN Training &amp; Kernel</h3>
                        <p>A neural network can be represented by the linearization:</p>
                        <div class="math-container">$$f(w, \ x) \simeq f(w_0 , \ x) \ + \ \nabla _w f( w_0 , \ x) ^T (w - w_0 )
                        $$</div>
                        <p>This Taylor expansion has the following properties:</p>
                        <ol>
                        <li>It is <strong><em>linear</em></strong> with respect to the weights $w$.</li>
                        <li>It is <strong><em>non-linear</em></strong> with respect to $x$.</li>
                        </ol>
                        <p>The gradient term $\nabla _w f( w_0 , \ x) ^T (w - w_0 )$ acts as a feature map that maps a non-linear data point $x$ to a useful space.</p>
                        <p>The corresponding kernel $K$ is defined as follows:</p>
                        <div class="math-container">
                            $$K(x, \ x') =  h_\text{NTK} = \{ \phi(x) , \ \phi (x') \} = \nabla _w f(w_0 , \ x) ^T \ \nabla _w f(w_0, \ x' )
                            $$
                        </div><br/>

                        <h3 id="2-2-2-gradient-based-training-kernel-regression"> Gradient-Based Training &amp; Kernel Regression</h3>
                        <p>The NTK can be found through gradient descent in the neural network. For a timestep $t$, gradient descent is expressed as:</p>
                        <div class="math-container">
                            $$w(t+1) \ = \ w(t) - \eta \nabla _w l. $$
                        </div>
                        <p>Subsequently, this can be derived as follows: </p>
                        <div class="math-container">
                            $$ { w(t+1)  \ - \ w(t) \over \eta } = -\nabla _w l \simeq {dw \over dt}.$$
                        </div>
                        <p>
                            With least squares (MSE) as the loss function,
                        </p>
                        <div class="math-container">
                            $$l(w) = {1 \over 2}  \| f(w, x ) - y \|^2,$$
                        </div>
                        <p> the gradient term $\nabla l$ with respect to the $w$ can be derived as </p>
                        <div class="math-container">
                            $$  \nabla _w l= \nabla _w  |f(w, x) - y |. $$
                        </div>
                        <p> Therefore, Neural network training via optimization can be represented by NTK kernel regression:</p>
                        <div class="math-container">
                            $$\begin{aligned}
                            {d \over dt } y(w) &= \nabla _w f(w, x) ^T \cdot {d \over dt }w \\
                            &= - \nabla _w f(w, x) ^T \cdot \nabla _w f(w, x) (f(w, x) - y) \\ &= -h_{\text{NTK}} (f(w,x) -y )
                            \end{aligned}$$
                        </div>
                        <p>Let $u=y(w)-y$, then the output residual at training iteration $t$ can be written as:</p>
                        <div class="math-container">$$ u(t) = u(0) \exp (-\eta h_{\text{NTK}} t )
                        $$</div><br/>

                        <h3 id="sec2.3">2.3. Spectral Bias of DNNs</h3>
                        <p>Based on the NTK approximation, the network&#39;s prediction after $t$ iterations for test data $\mathbf X_\text{test}$ is:</p>
                        <div class="math-container">
                            $$ \hat{\mathbf{y}}^{(t)} \simeq \mathbf{K}_{\text{test}} \mathbf{K}^{-1} ( \mathbf{I} - e^{-\eta \mathbf{K} t} ) \mathbf{y}$$
                        </div>
                        <p>For ideal training, $\mathbf K_\text{test} =  \mathbf K$. <em>i.e.,</em> equivalent to the last equation in 2.2.2.</p>
                        <p>By eigendecomposing $\mathbf K = \mathbf Q \mathbf \Lambda \mathbf Q^{\rm T}$, we obtain:</p>
                        <div class="math-container">
                            $$\begin{aligned}
                            \mathbf{Q}^{\rm T} (\hat{\mathbf{y}}^{(t)}
                            - \mathbf{y}) &\simeq \mathbf{Q}^{\rm T} ( \mathbf{K}_{\text{test}} \mathbf{K}^{-1} ( \mathbf{I} - e ^{-\eta \mathbf{K} t} ) \mathbf{y} - \mathbf{y} )) \\ 
                            & \simeq \mathbf{Q}^{\rm T} (  ( \mathbf{I} - e ^{-\eta \mathbf{K} t} ) \mathbf{y} - \mathbf{y} )) \\ 
                            & \simeq - e ^{-\eta \mathbf{\Lambda} t}   \mathbf{Q}^{\rm T} \mathbf y \quad (\because e ^{-\eta \mathbf{K} t} = \mathbf{Q} e ^{-\eta \mathbf{\Lambda} t} \mathbf{Q}^{\rm T} ) 
                            \end{aligned}$$
                        </div>
                        <p> In the above Equation, the exponential decay term decreases with the eigenvalue. It means <em>larger eigenvalues are learned first.</em> </p> 
                        <p> For example, in case of the image, large eigenvalues (in spectral domain) correspond to contours, so convergence to high-frequency components is slow without embedding in NeRF.</p>
                        <h2 id="sec3">3. Fourier Features for a Tunable Stationary Neural Tangent Kernel</h2>
                        <p>This section explores how Fourier Features embedding in the kernel space can address convergence issues for high-frequency components.</p>
                        <h3 id="sec3.1">3.1. Fourier-Featuring </h3>
                        <p>The Fourier-Feature mapping function $\gamma$ is defined as:</p>
                        <div class="math-container">$$\gamma (v) \  = \ \big [a_1 \cos (2 \pi b_1 ^T v), \dots , a_m \cos (2 \pi b_m ^T v), \ a_m \sin (2 \pi b_m ^T v ) \big ]^T
                        $$</div>
                        <ul>
                        <li><strong>Positional Encoding in Transformers:</strong> Adds spatial information to features in attention-based architectures, defined as:
                        $a_i =1, \ b_i = 10000^{i / d} , \ d : \text{dimension}$</li>
                        <li><strong>Positional Encoding in NeRF:</strong> Provides even distribution of low & high-frequency information in the input, defined as:
                        $a_i =1, \ b_i = 2^{i} {}$</li>
                        </ul>
                        
                        <p>The kernel induced by this mapping function is:</p>
                        <div class="math-container">
                            $$\begin{aligned}
                            K (\gamma (v_1 ) , \  \gamma (v_2) ) &= \gamma (v_1 ) ^T \gamma (v_2) \\ &= \sum _{j=1}^m a^2 _j \cos (2 \pi b_j ^T (v_1 -v_2) ) = h_\gamma (v_1 - v_2 )
                            \end{aligned}$$
                        </div>
                        <ul>
                        <li>remember: $\cos (\alpha - \beta ) = \cos \alpha \cos \beta  \ + \ \sin \alpha \sin \beta$</li>
                        </ul>
                        <p>This Fourier-feature kernel is a stationary function, meaning it is translation-invariant:</p>
                        <div class="math-container">$$h_\gamma( (v_1 +k )  -  (v_2 +k ) ) = h_\gamma (v_1  -  v_2 )
                            $$
                        </div>
                        <p>Coordinate-based MLPs use dense and uniform coordinate points as input. These must be <em>isotropic</em> to ensure global performance, meaning features should be extracted in all directions, not just specific ones.</p>
                        <p>This is why stationary properties that are location-invariant can improve performance. Positional encoding treats all equally distant relations from the coordinate system uniformly, enabling effective high-dimensional space reconstruction.</p>
                        <h3 id="sec3.2">3.2. NTK Kernel with Fourier-Featuring</h3>
                        <p>The NTK Fourier-featured kernel is:</p>
                        <p>$$K( \phi \circ \gamma (x) , \ \phi \circ \gamma (y) )
                        $$</p>
                        <p>Stationary kernel regression here equates to <strong><em>convolutional filtering with reconstruction</em></strong>, as the neural network approximates the convolution between synthetic kernels $K_\text{NTK}$ and $K_\gamma$ on data points $v_i$ and weights $w_i$.</p>
                        <p>Thus, the Fourier feature represented by NTK theory is:</p>
                        <div class="math-container">
                            $$f = ( h_\text{NTK} \circ h_\gamma ) * \sum_{i=1}^n w_i \delta _{v_i}$$
                        </div>
                        <p>where $\delta$ represents the direction delta.</p> 
                        <p>This expression indicates:</p>
                        <ol>
                        <li>A stationary filter $h_\gamma$ extracts information in a <strong><em>location-invariant</em></strong> manner.</li>
                        <li>Convolution, being the inverse Fourier transform of multiplication in frequency space, allows extraction of features across different frequencies in a multifaceted (yet location-invariant) way through components of specific frequencies directly embedded in $h_\gamma$.</li>
                        <li>A Neural Network, receiving Fourier-featured input, is equivalent to performing kernel regression by combining NTK and a stationary kernel.</strong></li>
                        </ol>
                        <video controls style="width: 100%;"><source src="https://bmild.github.io/fourfeat/img/lion_none_gauss_v1.mp4" type="video/mp4"></video>
                        <hr/>
                        <p>
                            You may also like, 
                        </p>


                    <div class="d-flex justify-content-end mb-4"><a class="btn btn-primary text-uppercase" href="../230202_ngp/">Next Post →</a></div>
                    <script src="https://giscus.app/client.js"
                            data-repo="hwan-h-heo/hwan-h-heo.io"
                            data-repo-id="R_kgDOMeZQdw"
                            data-category="General"
                            data-category-id="DIC_kwDOMeZQd84CiEMJ"
                            data-mapping="pathname"
                            data-strict="0"
                            data-reactions-enabled="1"
                            data-emit-metadata="0"
                            data-input-position="bottom"
                            data-theme="light_protanopia"
                            data-lang="en"
                            data-loading="lazy"
                            crossorigin="anonymous"
                            async>
                        </script>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="https://www.linkedin.com/in/hwan-heo-0905korea/">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="https://github.com/hwanhuh">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Hwan Heo</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../../../js/scripts.js"></script>
        <script src="https://unpkg.com/prismjs@1.28.0/components/prism-core.min.js"></script>
        <script src="https://unpkg.com/prismjs@1.28.0/plugins/autoloader/prism-autoloader.min.js"></script>
        <script>
            Prism.plugins.autoloader.languages_path = 'https://unpkg.com/prismjs@1.28.0/components/'
        </script>
    </body>
</html>
